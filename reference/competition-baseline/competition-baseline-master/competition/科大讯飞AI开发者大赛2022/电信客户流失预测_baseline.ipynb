{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.linear_model import SGDRegressor, LinearRegression, Ridge\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from gensim.models import Word2Vec\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, log_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['客户ID', '地理区域', '是否双频', '是否翻新机', '当前手机价格', '手机网络功能', '婚姻状况', '家庭成人人数',\n",
       "       '信息库匹配', '预计收入', '信用卡指示器', '当前设备使用天数', '在职总月数', '家庭中唯一订阅者的数量',\n",
       "       '家庭活跃用户数', '新手机用户', '信用等级代码', '平均月费用', '每月平均使用分钟数', '平均超额使用分钟数',\n",
       "       '平均超额费用', '平均语音费用', '数据超载的平均费用', '平均漫游呼叫数', '当月使用分钟数与前三个月平均值的百分比变化',\n",
       "       '当月费用与前三个月平均值的百分比变化', '平均掉线语音呼叫数', '平均丢弃数据呼叫数', '平均占线语音呼叫数',\n",
       "       '平均占线数据调用次数', '平均未接语音呼叫数', '未应答数据呼叫的平均次数', '尝试拨打的平均语音呼叫次数',\n",
       "       '尝试数据调用的平均数', '平均接听语音电话数', '平均完成的语音呼叫数', '完成数据调用的平均数', '平均客户服务电话次数',\n",
       "       '使用客户服务电话的平均分钟数', '一分钟内的平均呼入电话数', '平均三通电话数', '已完成语音通话的平均使用分钟数',\n",
       "       '平均呼入和呼出高峰语音呼叫数', '平均峰值数据调用次数', '使用高峰语音通话的平均不完整分钟数', '平均非高峰语音呼叫数',\n",
       "       '非高峰数据呼叫的平均数量', '平均掉线或占线呼叫数', '平均尝试调用次数', '平均已完成呼叫数', '平均呼叫转移呼叫数',\n",
       "       '平均呼叫等待呼叫数', '账户消费限额', '客户生命周期内的总通话次数', '客户生命周期内的总使用分钟数', '客户生命周期内的总费用',\n",
       "       '计费调整后的总费用', '计费调整后的总分钟数', '计费调整后的呼叫总数', '客户生命周期内平均月费用',\n",
       "       '客户生命周期内的平均每月使用分钟数', '客户整个生命周期内的平均每月通话次数', '过去三个月的平均每月使用分钟数',\n",
       "       '过去三个月的平均每月通话次数', '过去三个月的平均月费用', '过去六个月的平均每月使用分钟数', '过去六个月的平均每月通话次数',\n",
       "       '过去六个月的平均月费用', '是否流失'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练数据/测试数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [f for f in data.columns if f not in ['是否流失','客户ID']]\n",
    "\n",
    "train = data[data['是否流失'].notnull()].reset_index(drop=True)\n",
    "test = data[data['是否流失'].isnull()].reset_index(drop=True)\n",
    "\n",
    "x_train = train[features]\n",
    "x_test = test[features]\n",
    "\n",
    "y_train = train['是否流失']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "- 直接构建了一个函数，可以调用三种树模型，方便快捷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_model(clf, train_x, train_y, test_x, clf_name):\n",
    "    folds = 5\n",
    "    seed = 2022\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "\n",
    "    train = np.zeros(train_x.shape[0])\n",
    "    test = np.zeros(test_x.shape[0])\n",
    "\n",
    "    cv_scores = []\n",
    "\n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "        print('************************************ {} ************************************'.format(str(i+1)))\n",
    "        trn_x, trn_y, val_x, val_y = train_x.iloc[train_index], train_y[train_index], train_x.iloc[valid_index], train_y[valid_index]\n",
    "\n",
    "        if clf_name == \"lgb\":\n",
    "            train_matrix = clf.Dataset(trn_x, label=trn_y)\n",
    "            valid_matrix = clf.Dataset(val_x, label=val_y)\n",
    "\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc',\n",
    "                'min_child_weight': 5,\n",
    "                'num_leaves': 2 ** 5,\n",
    "                'lambda_l2': 10,\n",
    "                'feature_fraction': 0.7,\n",
    "                'bagging_fraction': 0.7,\n",
    "                'bagging_freq': 10,\n",
    "                'learning_rate': 0.2,\n",
    "                'seed': 2022,\n",
    "                'n_jobs':-1\n",
    "            }\n",
    "\n",
    "            model = clf.train(params, train_matrix, 50000, valid_sets=[train_matrix, valid_matrix], \n",
    "                              categorical_feature=[], verbose_eval=3000, early_stopping_rounds=200)\n",
    "            val_pred = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "            test_pred = model.predict(test_x, num_iteration=model.best_iteration)\n",
    "            \n",
    "            print(list(sorted(zip(features, model.feature_importance(\"gain\")), key=lambda x: x[1], reverse=True))[:20])\n",
    "                \n",
    "        if clf_name == \"xgb\":\n",
    "            train_matrix = clf.DMatrix(trn_x , label=trn_y)\n",
    "            valid_matrix = clf.DMatrix(val_x , label=val_y)\n",
    "            test_matrix = clf.DMatrix(test_x)\n",
    "            \n",
    "            params = {'booster': 'gbtree',\n",
    "                      'objective': 'binary:logistic',\n",
    "                      'eval_metric': 'auc',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 5,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.2,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2020,\n",
    "                      'nthread': 36,\n",
    "                      \"silent\": True,\n",
    "                      }\n",
    "            \n",
    "            watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]\n",
    "            \n",
    "            model = clf.train(params, train_matrix, num_boost_round=50000, evals=watchlist, verbose_eval=3000, early_stopping_rounds=200)\n",
    "            val_pred  = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit)\n",
    "            test_pred = model.predict(test_matrix , ntree_limit=model.best_ntree_limit)\n",
    "                 \n",
    "        if clf_name == \"cat\":\n",
    "            params = {'learning_rate': 0.2, 'depth': 5, 'l2_leaf_reg': 10, 'bootstrap_type': 'Bernoulli',\n",
    "                      'od_type': 'Iter', 'od_wait': 50, 'random_seed': 11, 'allow_writing_files': False}\n",
    "            \n",
    "            model = clf(iterations=20000, **params)\n",
    "            model.fit(trn_x, trn_y, eval_set=(val_x, val_y),\n",
    "                      cat_features=[], use_best_model=True, verbose=3000)\n",
    "            \n",
    "            val_pred  = model.predict(val_x)\n",
    "            test_pred = model.predict(test_x)\n",
    "            \n",
    "        train[valid_index] = val_pred\n",
    "        test = test_pred / kf.n_splits\n",
    "        cv_scores.append(roc_auc_score(val_y, val_pred))\n",
    "        \n",
    "        print(cv_scores)\n",
    "       \n",
    "    print(\"%s_scotrainre_list:\" % clf_name, cv_scores)\n",
    "    print(\"%s_score_mean:\" % clf_name, np.mean(cv_scores))\n",
    "    print(\"%s_score_std:\" % clf_name, np.std(cv_scores))\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_model(x_train, y_train, x_test):\n",
    "    lgb_train, lgb_test = cv_model(lgb, x_train, y_train, x_test, \"lgb\")\n",
    "    return lgb_train, lgb_test\n",
    "\n",
    "def xgb_model(x_train, y_train, x_test):\n",
    "    xgb_train, xgb_test = cv_model(xgb, x_train, y_train, x_test, \"xgb\")\n",
    "    return xgb_train, xgb_test\n",
    "\n",
    "def cat_model(x_train, y_train, x_test):\n",
    "    cat_train, cat_test = cv_model(CatBoostRegressor, x_train, y_train, x_test, \"cat\") \n",
    "    return cat_train, cat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************ 1 ************************************\n",
      "[LightGBM] [Info] Number of positive: 60072, number of negative: 59928\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009464 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10515\n",
      "[LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500600 -> initscore=0.002400\n",
      "[LightGBM] [Info] Start training from score 0.002400\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[3000]\ttraining's auc: 0.999516\tvalid_1's auc: 0.812108\n",
      "[6000]\ttraining's auc: 1\tvalid_1's auc: 0.832951\n",
      "[9000]\ttraining's auc: 1\tvalid_1's auc: 0.840899\n",
      "Early stopping, best iteration is:\n",
      "[11399]\ttraining's auc: 1\tvalid_1's auc: 0.843936\n",
      "[('当前设备使用天数', 23206.93494541943), ('当月使用分钟数与前三个月平均值的百分比变化', 18966.850461155176), ('客户生命周期内的平均每月使用分钟数', 13798.198653414845), ('每月平均使用分钟数', 13793.37155648321), ('在职总月数', 13514.45736033097), ('客户整个生命周期内的平均每月通话次数', 13169.779234770685), ('已完成语音通话的平均使用分钟数', 12717.158051796257), ('客户生命周期内的总费用', 12660.67002588883), ('当前手机价格', 12073.533231802285), ('当月费用与前三个月平均值的百分比变化', 12001.614468619227), ('计费调整后的总费用', 11994.650365594774), ('计费调整后的总分钟数', 11881.445307731628), ('使用高峰语音通话的平均不完整分钟数', 11772.639638844877), ('客户生命周期内的总使用分钟数', 11543.160580940545), ('过去六个月的平均每月使用分钟数', 11353.998523144051), ('客户生命周期内平均月费用', 11079.994449861348), ('客户生命周期内的总通话次数', 10965.192475471646), ('过去六个月的平均每月通话次数', 10816.046816661954), ('过去三个月的平均每月通话次数', 10654.587144132704), ('平均月费用', 10615.659350316972)]\n",
      "[0.8439362868562585]\n",
      "************************************ 2 ************************************\n",
      "[LightGBM] [Info] Number of positive: 59900, number of negative: 60100\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008806 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10528\n",
      "[LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499167 -> initscore=-0.003333\n",
      "[LightGBM] [Info] Start training from score -0.003333\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[3000]\ttraining's auc: 0.999531\tvalid_1's auc: 0.81211\n",
      "[6000]\ttraining's auc: 1\tvalid_1's auc: 0.832441\n",
      "[9000]\ttraining's auc: 1\tvalid_1's auc: 0.83953\n",
      "[12000]\ttraining's auc: 1\tvalid_1's auc: 0.84293\n",
      "Early stopping, best iteration is:\n",
      "[12210]\ttraining's auc: 1\tvalid_1's auc: 0.843058\n",
      "[('当前设备使用天数', 24020.498692663386), ('当月使用分钟数与前三个月平均值的百分比变化', 19773.12492423877), ('每月平均使用分钟数', 13641.919732686132), ('在职总月数', 13542.374755300581), ('客户整个生命周期内的平均每月通话次数', 13250.751761453226), ('客户生命周期内的平均每月使用分钟数', 13230.550698732957), ('已完成语音通话的平均使用分钟数', 12665.135287033394), ('当前手机价格', 12515.943285102025), ('计费调整后的总费用', 12446.485826000571), ('客户生命周期内的总费用', 12174.580246660858), ('当月费用与前三个月平均值的百分比变化', 12122.996504634619), ('使用高峰语音通话的平均不完整分钟数', 11753.57425396517), ('客户生命周期内的总使用分钟数', 11670.048939611763), ('计费调整后的总分钟数', 11564.826041478664), ('过去六个月的平均每月使用分钟数', 11223.45485602878), ('客户生命周期内的总通话次数', 11168.901827361435), ('过去六个月的平均每月通话次数', 11124.491803480312), ('过去三个月的平均每月通话次数', 10913.613902557641), ('客户生命周期内平均月费用', 10808.741903448477), ('计费调整后的呼叫总数', 10793.087901951745)]\n",
      "[0.8439362868562585, 0.8430577396278305]\n",
      "************************************ 3 ************************************\n",
      "[LightGBM] [Info] Number of positive: 60098, number of negative: 59902\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10513\n",
      "[LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500817 -> initscore=0.003267\n",
      "[LightGBM] [Info] Start training from score 0.003267\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[3000]\ttraining's auc: 0.999479\tvalid_1's auc: 0.816165\n",
      "[6000]\ttraining's auc: 1\tvalid_1's auc: 0.835227\n",
      "[9000]\ttraining's auc: 1\tvalid_1's auc: 0.842783\n",
      "Early stopping, best iteration is:\n",
      "[10972]\ttraining's auc: 1\tvalid_1's auc: 0.845106\n",
      "[('当前设备使用天数', 23554.02359988913), ('当月使用分钟数与前三个月平均值的百分比变化', 19450.002618733793), ('每月平均使用分钟数', 13781.198779758066), ('客户生命周期内的平均每月使用分钟数', 13459.828927565366), ('在职总月数', 13450.310572762042), ('客户整个生命周期内的平均每月通话次数', 12807.892476923764), ('已完成语音通话的平均使用分钟数', 12764.867111746222), ('客户生命周期内的总费用', 12400.86265109852), ('当前手机价格', 12370.400694530457), ('计费调整后的总费用', 12057.831106703728), ('当月费用与前三个月平均值的百分比变化', 11742.217323374003), ('计费调整后的总分钟数', 11737.426330137998), ('客户生命周期内的总使用分钟数', 11546.544992171228), ('过去六个月的平均每月通话次数', 11189.07267446071), ('使用高峰语音通话的平均不完整分钟数', 11077.357912018895), ('客户生命周期内平均月费用', 11054.351627696306), ('过去六个月的平均每月使用分钟数', 11045.627827014774), ('客户生命周期内的总通话次数', 10956.346342962235), ('过去三个月的平均每月通话次数', 10770.111043587327), ('过去三个月的平均每月使用分钟数', 10649.836913790554)]\n",
      "[0.8439362868562585, 0.8430577396278305, 0.8451055611157318]\n",
      "************************************ 4 ************************************\n",
      "[LightGBM] [Info] Number of positive: 59934, number of negative: 60066\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010539 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10521\n",
      "[LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499450 -> initscore=-0.002200\n",
      "[LightGBM] [Info] Start training from score -0.002200\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[3000]\ttraining's auc: 0.999563\tvalid_1's auc: 0.811568\n",
      "[6000]\ttraining's auc: 1\tvalid_1's auc: 0.833085\n",
      "[9000]\ttraining's auc: 1\tvalid_1's auc: 0.840568\n",
      "Early stopping, best iteration is:\n",
      "[10815]\ttraining's auc: 1\tvalid_1's auc: 0.842836\n",
      "[('当前设备使用天数', 23250.61392029375), ('当月使用分钟数与前三个月平均值的百分比变化', 19292.30032589659), ('客户生命周期内的平均每月使用分钟数', 13766.847205281258), ('每月平均使用分钟数', 13647.102678723633), ('在职总月数', 13616.28791507706), ('客户整个生命周期内的平均每月通话次数', 12942.528880607337), ('客户生命周期内的总费用', 12354.855169367045), ('已完成语音通话的平均使用分钟数', 12344.891597270966), ('当前手机价格', 12116.039827257395), ('计费调整后的总费用', 11973.962297733873), ('客户生命周期内的总使用分钟数', 11728.554908126593), ('当月费用与前三个月平均值的百分比变化', 11600.252432178706), ('计费调整后的总分钟数', 11484.329358864576), ('使用高峰语音通话的平均不完整分钟数', 11409.022325478494), ('客户生命周期内平均月费用', 11246.711442269385), ('客户生命周期内的总通话次数', 11189.995209667832), ('过去六个月的平均每月通话次数', 10974.940132912248), ('过去六个月的平均每月使用分钟数', 10896.997217286378), ('计费调整后的呼叫总数', 10649.78157223016), ('过去三个月的平均每月使用分钟数', 10490.423435229808)]\n",
      "[0.8439362868562585, 0.8430577396278305, 0.8451055611157318, 0.8428364570863798]\n",
      "************************************ 5 ************************************\n",
      "[LightGBM] [Info] Number of positive: 60164, number of negative: 59836\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 10530\n",
      "[LightGBM] [Info] Number of data points in the train set: 120000, number of used features: 67\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501367 -> initscore=0.005467\n",
      "[LightGBM] [Info] Start training from score 0.005467\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[3000]\ttraining's auc: 0.999484\tvalid_1's auc: 0.812766\n",
      "[6000]\ttraining's auc: 1\tvalid_1's auc: 0.833513\n",
      "[9000]\ttraining's auc: 1\tvalid_1's auc: 0.840732\n",
      "[12000]\ttraining's auc: 1\tvalid_1's auc: 0.843099\n",
      "Early stopping, best iteration is:\n",
      "[12570]\ttraining's auc: 1\tvalid_1's auc: 0.84359\n",
      "[('当前设备使用天数', 23424.84182724543), ('当月使用分钟数与前三个月平均值的百分比变化', 20145.600453276187), ('客户生命周期内的平均每月使用分钟数', 13742.811106188223), ('每月平均使用分钟数', 13542.054858371615), ('在职总月数', 13331.132492953911), ('客户整个生命周期内的平均每月通话次数', 13032.698692249134), ('已完成语音通话的平均使用分钟数', 12668.222773976624), ('当前手机价格', 12389.838216289878), ('客户生命周期内的总费用', 12380.491549521685), ('使用高峰语音通话的平均不完整分钟数', 12170.956499611959), ('计费调整后的总费用', 12101.883673759177), ('客户生命周期内的总使用分钟数', 11935.202114250511), ('当月费用与前三个月平均值的百分比变化', 11643.637906264514), ('计费调整后的总分钟数', 11638.548691518605), ('客户生命周期内平均月费用', 11420.638470709324), ('客户生命周期内的总通话次数', 11384.500305030495), ('过去六个月的平均每月使用分钟数', 11285.595895411447), ('过去六个月的平均每月通话次数', 10763.348691094667), ('过去三个月的平均每月通话次数', 10488.567783802748), ('平均月费用', 10480.520216416568)]\n",
      "[0.8439362868562585, 0.8430577396278305, 0.8451055611157318, 0.8428364570863798, 0.8435898444055294]\n",
      "lgb_scotrainre_list: [0.8439362868562585, 0.8430577396278305, 0.8451055611157318, 0.8428364570863798, 0.8435898444055294]\n",
      "lgb_score_mean: 0.843705177818346\n",
      "lgb_score_std: 0.000800204784264596\n"
     ]
    }
   ],
   "source": [
    "lgb_train, lgb_test = lgb_model(x_train, y_train, x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************ 1 ************************************\n",
      "[19:57:59] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.62651\teval-auc:0.61938\n",
      "[3000]\ttrain-auc:0.99330\teval-auc:0.79046\n",
      "[6000]\ttrain-auc:1.00033\teval-auc:0.80912\n",
      "[9000]\ttrain-auc:1.00043\teval-auc:0.81392\n",
      "[10249]\ttrain-auc:1.00043\teval-auc:0.81494\n",
      "[0.8150796492074857]\n",
      "************************************ 2 ************************************\n",
      "[20:15:06] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.62530\teval-auc:0.62010\n",
      "[3000]\ttrain-auc:0.99286\teval-auc:0.79012\n",
      "[6000]\ttrain-auc:0.99995\teval-auc:0.80709\n",
      "[8304]\ttrain-auc:1.00005\teval-auc:0.81092\n",
      "[0.8150796492074857, 0.8110077629356971]\n",
      "************************************ 3 ************************************\n",
      "[20:28:24] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.62804\teval-auc:0.61815\n",
      "[3000]\ttrain-auc:0.99321\teval-auc:0.78782\n",
      "[6000]\ttrain-auc:1.00062\teval-auc:0.80561\n",
      "[9000]\ttrain-auc:1.00071\teval-auc:0.81071\n",
      "[10611]\ttrain-auc:1.00071\teval-auc:0.81213\n",
      "[0.8150796492074857, 0.8110077629356971, 0.8121744465736198]\n",
      "************************************ 4 ************************************\n",
      "[20:44:24] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.62600\teval-auc:0.62415\n",
      "[3000]\ttrain-auc:0.99286\teval-auc:0.79084\n",
      "[6000]\ttrain-auc:0.99948\teval-auc:0.80703\n",
      "[9000]\ttrain-auc:0.99958\teval-auc:0.81178\n",
      "[9092]\ttrain-auc:0.99958\teval-auc:0.81162\n",
      "[0.8150796492074857, 0.8110077629356971, 0.8121744465736198, 0.8118131510604176]\n",
      "************************************ 5 ************************************\n",
      "[20:57:31] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:573: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-auc:0.62763\teval-auc:0.62610\n",
      "[3000]\ttrain-auc:0.99314\teval-auc:0.78956\n",
      "[6000]\ttrain-auc:0.99984\teval-auc:0.80622\n",
      "[9000]\ttrain-auc:0.99993\teval-auc:0.81061\n",
      "[10613]\ttrain-auc:0.99993\teval-auc:0.81160\n",
      "[0.8150796492074857, 0.8110077629356971, 0.8121744465736198, 0.8118131510604176, 0.811683435983383]\n",
      "xgb_scotrainre_list: [0.8150796492074857, 0.8110077629356971, 0.8121744465736198, 0.8118131510604176, 0.811683435983383]\n",
      "xgb_score_mean: 0.8123516891521205\n",
      "xgb_score_std: 0.0014153377154753038\n"
     ]
    }
   ],
   "source": [
    "xgb_train, xgb_test = xgb_model(x_train, y_train, x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************ 1 ************************************\n",
      "0:\tlearn: 0.4955489\ttest: 0.4954619\tbest: 0.4954619 (0)\ttotal: 104ms\tremaining: 34m 45s\n",
      "3000:\tlearn: 0.3769726\ttest: 0.4483572\tbest: 0.4483572 (3000)\ttotal: 25.1s\tremaining: 2m 22s\n",
      "6000:\tlearn: 0.3209359\ttest: 0.4391546\tbest: 0.4391520 (5999)\ttotal: 53.4s\tremaining: 2m 4s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.4360869428\n",
      "bestIteration = 7499\n",
      "\n",
      "Shrink model to first 7500 iterations.\n",
      "[0.78868229695141]\n",
      "************************************ 2 ************************************\n",
      "0:\tlearn: 0.4953117\ttest: 0.4954092\tbest: 0.4954092 (0)\ttotal: 15.4ms\tremaining: 5m 8s\n",
      "3000:\tlearn: 0.3763302\ttest: 0.4490481\tbest: 0.4490378 (2981)\ttotal: 25.6s\tremaining: 2m 24s\n",
      "6000:\tlearn: 0.3196365\ttest: 0.4402621\tbest: 0.4402621 (6000)\ttotal: 52s\tremaining: 2m 1s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.4361341716\n",
      "bestIteration = 8001\n",
      "\n",
      "Shrink model to first 8002 iterations.\n",
      "[0.78868229695141, 0.7897985044313038]\n",
      "************************************ 3 ************************************\n",
      "0:\tlearn: 0.4954711\ttest: 0.4955905\tbest: 0.4955905 (0)\ttotal: 13.8ms\tremaining: 4m 36s\n",
      "3000:\tlearn: 0.3763265\ttest: 0.4477431\tbest: 0.4477431 (3000)\ttotal: 24.9s\tremaining: 2m 21s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.4406746798\n",
      "bestIteration = 5128\n",
      "\n",
      "Shrink model to first 5129 iterations.\n",
      "[0.78868229695141, 0.7897985044313038, 0.7788144016087264]\n",
      "************************************ 4 ************************************\n",
      "0:\tlearn: 0.4955798\ttest: 0.4955669\tbest: 0.4955669 (0)\ttotal: 12.5ms\tremaining: 4m 10s\n",
      "3000:\tlearn: 0.3768704\ttest: 0.4486424\tbest: 0.4486421 (2997)\ttotal: 25s\tremaining: 2m 21s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.4426386429\n",
      "bestIteration = 4903\n",
      "\n",
      "Shrink model to first 4904 iterations.\n",
      "[0.78868229695141, 0.7897985044313038, 0.7788144016087264, 0.7744056829683829]\n",
      "************************************ 5 ************************************\n",
      "0:\tlearn: 0.4955262\ttest: 0.4956471\tbest: 0.4956471 (0)\ttotal: 12.6ms\tremaining: 4m 12s\n",
      "3000:\tlearn: 0.3761659\ttest: 0.4494234\tbest: 0.4494234 (3000)\ttotal: 25.3s\tremaining: 2m 23s\n",
      "6000:\tlearn: 0.3202277\ttest: 0.4407377\tbest: 0.4407330 (5999)\ttotal: 51.1s\tremaining: 1m 59s\n",
      "9000:\tlearn: 0.2781913\ttest: 0.4347233\tbest: 0.4347168 (8998)\ttotal: 1m 17s\tremaining: 1m 34s\n",
      "Stopped by overfitting detector  (50 iterations wait)\n",
      "\n",
      "bestTest = 0.4323322625\n",
      "bestIteration = 10483\n",
      "\n",
      "Shrink model to first 10484 iterations.\n",
      "[0.78868229695141, 0.7897985044313038, 0.7788144016087264, 0.7744056829683829, 0.7982800693357867]\n",
      "cat_scotrainre_list: [0.78868229695141, 0.7897985044313038, 0.7788144016087264, 0.7744056829683829, 0.7982800693357867]\n",
      "cat_score_mean: 0.785996191059122\n",
      "cat_score_std: 0.0084674009574612\n"
     ]
    }
   ],
   "source": [
    "cat_train, cat_test = cat_model(x_train, y_train, x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['是否流失'] = lgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['客户ID','是否流失']].to_csv('test_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
