---
title: "基于词典法的财经新闻情感分析"
output: html_document
---


## 准备工作

```{r setup}
# 清除工作环境
cat("\014"); rm(list=ls())

library(kableExtra)   # 加载kableExtra包，用于在html中展示滚动表格
library(jiebaRD)      # 加载R包 用于jieba分词
library(jiebaR)       # 加载R包 用于jieba分词 
library(reshape2)     # 加载包用于整合分词后的文本
library(wordcloud2)   # 加载程序包用于绘制词云图
library(ggplot2)      # 加载程序包进行图形绘制
```

## 任务一：数据读入

设置好你的工作路径，找到评论数据集data.csv。读入该数据，命名为news_data。使用head()查看数据的基本情况。

其中，index表示的是该新闻的编号，title表示的是该新闻的标题，time表示的是该新闻发表的时间，author表示的是该新闻的作者或发表机构，content表示的是该新闻的具体内容。

由于新闻文本较长，此处使用kableExtra包进行滚动文本查看。

```{r read data}
news_data <- read.csv("./data.csv")  #读入comments数据
kbl(head(news_data, 6)) %>%          #查看数据
  kable_paper() %>%
  scroll_box(height = "500px")
```

## 任务二：词典准备

使用词典法进行情感分析，主要依赖已经被标记了正负向的情感词典。常用的中文情感词典有：（1）[知网褒贬义词典](http://www.keenage.com/html/c_bulletin_2007.htm)、（2）[清华大学李军褒贬义词典](http://nlp.csai.tsinghua.edu.cn/site2/index.php/13-sms)、（3）台湾中央研究院开发的[中文情感记性词典（NTUSD）](http://academiasinicanlplab.github.io/)。

上述三个词典的词分别存储在文件夹知网Hownet、清华大学——李军中文褒贬义词典、台湾大学NTUSD。读入上述三个词典，将正向词和负向词分别整合成一个向量并去重。展示两个向量的前6个词。


```{r dictionary preparation}

# NTUSD正负向词语
positive.NTUSD <- read.csv("./台湾大学NTUSD/NTUSD_positive_simplified.txt", encoding = "UTF-8", header = FALSE)
negative.NTUSD <- read.csv("./台湾大学NTUSD/NTUSD_negative_simplified.txt", encoding = "UTF-8", header = FALSE)

# 清华大学李军褒贬义词典
positive.Tsinghua <- read.csv("./清华大学——李军中文褒贬义词典/tsinghua.positive.gb.txt", fileEncoding = "GBK", header = FALSE)
negative.Tsinghua <- read.csv("./清华大学——李军中文褒贬义词典/tsinghua.negative.gb.txt", fileEncoding = "GBK", header = FALSE)

# 知网褒贬义词典
positive.Hownet.1 <- read.csv("./知网Hownet/正面评价词语（中文）.txt", fileEncoding = "GBK", header = TRUE)
positive.Hownet.2 <- read.csv("./知网Hownet/正面情感词语（中文）.txt", fileEncoding = "GBK", header = TRUE)
negative.Hownet.1 <- read.csv("./知网Hownet/负面评价词语（中文）.txt", fileEncoding = "GBK", header = TRUE)
negative.Hownet.2 <- read.csv("./知网Hownet/负面情感词语（中文）.txt", fileEncoding = "GBK", header = TRUE)

# 合并上述三个词典
# positive合并以后为11935个正面词汇（有1008个重复的词语，已使用unique去除）
positive <- unique(c(positive.Tsinghua$V1, 
                     positive.NTUSD$V1, 
                     positive.Hownet.1$中文正面评价词语.3730, 
                     positive.Hownet.2$中文正面情感词语.836))
# negative合并以后为15303个负面词汇（有1810个重复的词语）
negative <- unique(c(negative.Tsinghua$V1, 
                     negative.NTUSD$V1, 
                     negative.Hownet.1$中文负面评价词语.3116, 
                     negative.Hownet.2$中文负面情感词语.1254))

add_dict <- data.frame(c(positive, negative))

# 写到本地
write.table(file = "./add_dict.txt", add_dict, row.names = FALSE, col.names = FALSE, fileEncoding = "UTF-8", quote = FALSE)

head(positive)
head(negative)
```

## 任务三：文本分词

提取数据中的新闻正文评论所在列，并命名为news；对news进行分词。你需要用到程序包jiebaRD和jiebaR。加载这些包后，使用分词器worker对全部评论数据进行逐行分词。你需要去除停用词，停用词见数据集stop_words.txt。同时，还需要设置**自定义词典**，自定义词典即任务二合并生成的情感词典add_dict。

```{r word segmentation}
news <- news_data$content       # 提取评论所在的列
  
stoppath <- "./stop_words.txt"  # 设置停用词所在目录
news <- gsub("[0-9]","",news)   # 新闻中出现有大量的数字，将数字去除
dictpath <- "./add_dict.txt"    #设置自定义字典所在目录

# 初始化分词器worker，在分词的时候去停用词，加自定义词典
cutter = worker('tag', bylines = TRUE, stop_word = stoppath, user = dictpath, symbol = F) 
res=cutter[news]                # 进行分词,这步会比较慢
res[[1]][1:20]                  # 展示部分分词情况
```

## 任务四：词云图绘制

从分词后的全部文本中进行高频词提取。你首先需要将全部分词后的文本整合成一列，这一步可以尝试使用do.call函数来提高运算效率；然后对整合后的数据使用table函数进行词频统计；最后，查看词频最高的前100个词，然后用这些词画个词云图，你可以使用程序包wordcloud2。在top100高频词中，你都看到了哪些有意思的词呢？

```{r wordcloud}
##整合分词后的文本
text <- lapply(res,as.matrix)                          #将text从list转换为matrix格式
text <- as.data.frame(do.call(rbind, text),stringsAsFactors=F)
# 将每行文本的分词结果逐一排列起来
freq <- as.data.frame(table(text),stringsAsFactors=F)  #计算每个词出现的频率
freq <- freq[order(-freq[,2]),]                        #按频率从多到少排列
# 绘制词云图
top100 <- freq[1:100,]                                 #提取所有评论中前100个高频词
wordcloud2(top100)                                     #绘制词云图
```

解读示例：从词云图中我们可以看出出现频率最高的是公司、中国、市场、发展、增长等，可见新闻中大部分都是与企业发展、国家动态有关的新闻。

## 任务五：文本极性判断

为了判断每条文本的情感极性，首先统计每条文本中包括的“正向词语”和“负向词语”的个数，然后根据两类词语的个数情况计算文本的情感极性得分。在计算极性得分时可以采用多种方法。

我们令N=负向情感倾向词的个数，P=正向情感倾向词的个数，定义：情感绝对得分=P-N，情感相对得分=(P-N)/(P+N)。以上述两种方法为例，计算数据中每条文本的情感得分，然后使用直方图展示情感极性得分的分布情况。

接下来查看前几个文本的判断情况，我们发现，大部分的情感倾向判断是准确的。

```{r polarity}
for (i in 1:length(news_data[,1])) {
  news_data$negative.count[i] <- sum(res[[i]] %in% negative)
  news_data$positive.count[i] <- sum(res[[i]] %in% positive)
  news_data$"情感绝对得分"[i] <- news_data$positive.count[i] - news_data$negative.count[i]
  news_data$"情感相对得分"[i] <- (news_data$positive.count[i] - news_data$negative.count[i]) / (news_data$positive.count[i] + news_data$negative.count[i])
  if (news_data$negative.count[i] > news_data$positive.count[i])
    news_data$polarity[i] <- "负向"
  else if (news_data$negative.count[i] < news_data$positive.count[i])
    news_data$polarity[i] <- "正向"
  else
    news_data$polarity[i] <- "中性"
}
kbl(head(news_data[,-5], 100)) %>%          #查看数据
  kable_paper() %>%
  scroll_box(height = "500px")
```


```{r plot, warning=FALSE}
score1.density <- ggplot(NULL, aes(news_data$"情感绝对得分")) +
  geom_histogram(binwidth = 50, colour='#e77f67', fill='#f7d794') +
  ggtitle("情感绝对得分分布直方图") +
  xlab("情感绝对得分") +
  ylab("计数") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

score1.density

score2.density <- ggplot(NULL, aes(news_data$"情感相对得分")) +
  geom_histogram(binwidth = 0.1, colour='#e77f67', fill='#f7d794') +
  ggtitle("情感相对得分分布直方图") +
  xlab("情感相对得分") +
  ylab("计数") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

score2.density
```

- 从情感得分的分布来看，情感偏正向的文章偏多。情感绝对得分由于呈现明显的右偏分布。

## 任务六：文本极性展示

展示情感得分最高和最低的几条文本

- 情感绝对得分最高的5条正向新闻

```{r result1}
positive.max.1 <- news_data[order(news_data$情感绝对得分, decreasing = TRUE),][1:5,]

kbl(positive.max.1) %>%          #查看数据
  kable_paper() %>%
  scroll_box(height = "500px")
```

- 情感绝对得分最低的5条负向新闻

```{r result2}
negative.max.1 <- news_data[order(news_data$情感绝对得分, decreasing = FALSE),][1:5,]

kbl(negative.max.1) %>%          #查看数据
  kable_paper() %>%
  scroll_box(height = "500px")
```


- 情感相对得分最高的5条正向新闻

```{r result3}
positive.max.2 <- news_data[order(news_data$情感相对得分, news_data$positive.count, decreasing = TRUE),][1:5,]

kbl(positive.max.2) %>%          #查看数据
  kable_paper() %>%
  scroll_box(height = "500px")
```

- 情感相对得分最低的5条负向新闻

```{r result4}
negative.max.2 <- news_data[order(news_data$情感相对得分, -news_data$negative.count, decreasing = FALSE),][1:5,]

kbl(negative.max.2) %>%          #查看数据
  kable_paper() %>%
  scroll_box(height = "500px")
```

