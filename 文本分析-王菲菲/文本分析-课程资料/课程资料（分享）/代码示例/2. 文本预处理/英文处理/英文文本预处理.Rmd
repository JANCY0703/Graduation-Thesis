---
title: "英文文本预处理"
date: ""
output:
  rmdformats::readthedown:
    highlight: kate
    number_sections: true
editor_options: 
  chunk_output_type: console
---


```{r setup, include=FALSE}
library(knitr)
library(rmdformats)
library(showtextdb)
library(showtext)
showtext_auto(enable=T)
opts_chunk$set(message=FALSE,
               warning=FALSE)
```


# 安装R包

```{r}
library("quanteda")
library("textstem")
library("qdap")
library("jiebaR")
library("wordcloud2")
library("RColorBrewer")
```


# 读入并提取文本数据

```{r}
movie <-readxl::read_xlsx("./movie reviews.xlsx") #读入xlsx文件
kable(movie[1,]) #查看第一行，发现评论在“review”列
```

```{r}
review <- movie[which(movie$`film name`=="Ant-Man"),]$review #将电影“Ant-Man”的评论保存到review变量中
```


# 除去HTML标签

```{r}
for (i in 1:length(review)){
  review[i] <- gsub("<.*?>","",review[i]) #除去<>包围的关键词
  review[i] <- gsub("Â.*","",review[i]) #除去特殊符号Â
  review[i] <- gsub("Ã.*","",review[i]) #除去特殊符号Ã
}
```


# 词形还原

```{r}
review[1] #查看词形还原前的第一条评论
review <- lemmatize_strings(review) #实现词形还原
review[1] #查看词形还原后的第一条评论
```


# 分词

```{r}
review <- quanteda::tokens(review) #实现分词
review[1] #查看分词后的第一条评论
class(review) #查看分词后的变量类型
```


# 文本数据清洗
 
## 使用tokens()进行清洗

```{r}
review <- quanteda::tokens(review,remove_punct = TRUE,remove_symbols = TRUE,remove_numbers = TRUE,remove_url = TRUE,remove_separators = TRUE) #去除标点、符号、数字、url网址和分隔符
```

## 去掉专有名词

专有名词list下载网址： https://onlymyenglish.com/proper-noun-list/

```{r}
propernames <-readLines("./propernames.txt") #读入专有名词表
review <- quanteda::tokens_remove(review,propernames) #去掉专有名词
```


# 统一小写

```{r}
review <- quanteda::tokens_tolower(review) #统一小写
review[1] #查看统一小写后的第一条评论
```


# 去掉常用或罕见的词

## 去掉停用词

```{r}
review1 <- tokens_remove(review,stopwords()) #去掉停用词
stopwords(language = "en") #查看停用词表
```

## 去掉出现频率小于k的词

```{r}
#以k=10为例
freq1 <-  freq(unlist(review)) #提取词频
frwords <- freq1[which(freq1$freq>=10),]$char #提取频率高于等于10的词
review2 <- tokens_keep(review,frwords) #保留频率高于等于10的词，去掉频率小于10的词
```

## 独热编码

```{r}
mydfm <- dfm(review) #转换为dfm类型
mydfmoh <- dfm_weight(mydfm,scheme = "boolean") #实现独热编码
mydfmoh #查看独热编码结果
```

## 根据TF-IDF去掉词

```{r}
f <- as.numeric(featfreq(mydfm)) #提取各词的频数
tf <- f/sum(f) #计算TF值
iv <- rep(0,nfeat(mydfmoh)) #生成一个零向量
for (i in 1:nfeat(mydfmoh)){
  iv[i] = sum(as.numeric(mydfmoh[,i])) #计算各词出现在多少个评论中
}
idf <- log10(nrow(mydfmoh)/iv) #计算IDF值
tfidf <- tf*idf #计算TF-IDF值
voclist <- featnames(mydfmoh) #提取单词列表
ti <- cbind(voclist,tf,idf,tfidf) #合并单词列表、TF值、IDF值以及TF-IDF值
frwords <- voclist[which(tfidf>0.003)] #提取TF-IDF大于0.003的词
review3 <- tokens_keep(review,frwords) #仅保留TF—IDF大于0.003的词
```


# 检查拼写

```{r}
review <- review1 #此时假设上一步采用了去掉停用词的方法
chreview <- unlist(review) #将review转为character类型
ck <- check_spelling(chreview) #检查拼写
#ck$suggestion #查看建议
review <- tokens_remove(review,ck$not.found) #去掉拼写错误的词
```


# 词干化

```{r}
review[1] #查看词干化前的第一条评论
reviewst <- tokens_wordstem(review) #提取词干
reviewst[1] #查看词干化后的第一条评论
```


# N元模型

## 一元语法分词

```{r}
sggram <- review #实现一元语法分词
sgdfm <- dfm(review) #转换为dfm类型
kable(t(topfeatures(sgdfm,10))) #展示词频最高的10个词
```

## 二元语法分词

```{r}
bigram <- tokens_ngrams(review,n=2) #实现二元语法分词
bidfm <- dfm(bigram) #转换为dfm类型
kable(t(topfeatures(bidfm,5))) #展示词频最高的5个词
```

## 三元语法分词

```{r}
trigram <- tokens_ngrams(review,n=3) #实现三元语法分词
tridfm <- dfm(trigram) #转换为dfm类型
kable(t(topfeatures(tridfm,3))) #展示词频最高的3个词
```


# 绘制词云图

```{r}
sgfreq <- freq(unlist(review)) #提取词频
sgfreq <-  sgfreq[order(-sgfreq[,2]),] #将词频按从高到低排列
wordcloud2(sgfreq[1:20,],size = 0.6, color = brewer.pal(n = 20, name = "PiYG"),backgroundColor = "white",shape = "circle") #绘制前20个词的词云图
```


