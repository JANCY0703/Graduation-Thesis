---
title: "Word2Vec for Chinese Text Analysis"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
    self_contained: true
    code_folding: show
    toc: 1
    number_sections: true
---

```{r setup, include=FALSE}
library(knitr)
library(rmdformats)
library(word2vec)
library(jiebaR)

## Global options
options(max.print="1e5")
opts_chunk$set(echo=TRUE,
	             cache=FALSE,
               prompt=FALSE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)

opts_knit$set(width=75)

colors <- c('#f3a683', 
            '#f7d794', 
            '#778beb', 
            '#e77f67', 
            '#cf6a87', 
            '#f19066', 
            '#f5cd79', 
            '#546de5', 
            '#e15f41', 
            '#c44569')
```


# 数据读入

```{r}
data.Chinese <- read.csv2("./labeled-corpus-sampled.csv", header = TRUE, sep = ",", encoding = "UTF-8")
texts <- data.Chinese$text
```

# 分词

```{r}
stoppath <- "./cn_stopwords.txt"  # 设置停用词所在目录
texts <- gsub("[0-9]","",texts)   # 将数字去除

# 初始化分词器worker，在分词的时候去停用词，加自定义词典
cutter = worker('tag', bylines = TRUE, stop_word = stoppath, symbol = F) 
res = cutter[texts]                # 进行分词,这步会比较慢
res[[1]]                  # 展示部分分词情况
```

-   对分词结果重新组合，方便导入word2vec函数

```{r}
# 将分词结果进行重新组织，词语与词语之间用空格分隔 
strs <- rep("", length(res))
for(i in 1:length(res)) {
  str <- ""
  for (j in 1:length(res[[i]])) {
    str <- paste(str, res[[i]][j], sep = " ")
  }
  strs[i] <- str
}

```

# 模型训练

```{r}
model <- word2vec(strs, type = "skip-gram", dim = 50, encoding = "UTF-8")

# 寻找距离“涨”最近的10个词
predict(model, "涨", top_n = 10)

model <- word2vec(strs, type = "cbow", dim = 50, encoding = "UTF-8")

predict(model, "涨", top_n = 10)
```
